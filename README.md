# Connect 4 Reinforcement Learning Project

A comprehensive Connect 4 implementation with advanced reinforcement learning agents, specifically designed to explore and solve catastrophic forgetting in deep RL. Features state-of-the-art techniques including Dueling DQN, anti-catastrophic forgetting measures, and strategic reward systems.

## ğŸš€ Key Features

### Advanced RL Techniques
- **Dueling DQN Architecture**: Separates state value from action advantages for better learning
- **Anti-Catastrophic Forgetting**: Progressive curriculum learning with experience replay diversity
- **Strategic Reward System**: Amplified intermediate rewards (2.0-5.0x) for tactical learning
- **Q-value Explosion Prevention**: Comprehensive stabilization with gradient clipping and clamping

### Training Stability
- **Smooth Curriculum Transitions**: 4000-episode overlap periods prevent performance drops
- **Polyak Averaging**: Conservative target network updates (Ï„=0.001)
- **Reservoir Sampling**: Maintains experience diversity across training phases
- **Learning Rate Warmup**: Gradual increase prevents early training instability

## Project Structure

```
forza_quattro/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ CLAUDE.md                         # Development guidelines
â”œâ”€â”€ game/
â”‚   â””â”€â”€ board.py                      # Core Connect 4 game logic
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ random_agent.py               # Random baseline agent
â”‚   â”œâ”€â”€ heuristic_agent.py            # Strategic rule-based agent
â”‚   â””â”€â”€ double_dqn_agent.py           # Advanced Deep RL agent
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ reward_system.py              # Enhanced strategic rewards
â”‚   â”œâ”€â”€ training_monitor.py           # Training visualization and logging
â”‚   â”œâ”€â”€ double_dqn_train.py           # Standard training pipeline
â”‚   â””â”€â”€ double_dqn_train_advanced.py  # Advanced training configurations
â”œâ”€â”€ train_fixed_double_dqn.py         # Main training script with all fixes
â”œâ”€â”€ fix_qvalue_learning.py            # Configuration for Q-value stability
â”œâ”€â”€ interactive_game.py               # Human vs AI gameplay
â”œâ”€â”€ evaluate_agents.py                # Agent performance evaluation
â”œâ”€â”€ docs/                             # Development documentation
â”œâ”€â”€ models_fixed/                     # Trained model checkpoints
â””â”€â”€ logs_fixed/                       # Training logs and plots
```

## Installation

1. Clone or download this project
2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ® Quick Start

### Train Advanced Double DQN Agent
```bash
python train_fixed_double_dqn.py
```

### Play Against AI
```bash
python interactive_game.py
```

### Evaluate Agent Performance
```bash
python evaluate_agents.py
```

### Monitor Training Progress
```bash
# View real-time logs
tail -f logs_fixed/double_dqn_log.csv

# Check training plots
ls logs_fixed/*.png
```

## ğŸ§  Technical Details

### Enhanced Reward System
- **Terminal Rewards**: Win (+10.0), Loss (-10.0), Draw (0.0)
- **Strategic Rewards** (Amplified 10-25x for learning visibility):
  - Blocked Opponent: +2.0 (was 0.1)
  - Played Winning Move: +3.0 (was 0.2)
  - Missed Block: -3.0 (was -0.1)
  - Missed Win: -5.0 (was -0.2)

### Dueling DQN Architecture
```
Network (256 hidden units):
â”œâ”€â”€ Shared Features: 84 â†’ 256 â†’ 256 (dropout 0.1)
â”œâ”€â”€ Value Stream: 256 â†’ 128 â†’ 1
â”œâ”€â”€ Advantage Stream: 256 â†’ 128 â†’ 7
â””â”€â”€ Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
```

### Training Configuration
- **Learning Rate**: 5e-5 (ultra-conservative)
- **Discount Factor**: 0.95 (prevents Q-value explosion)
- **Batch Size**: 64
- **Buffer Size**: 200,000
- **Target Update**: Every 2000 steps with Polyak averaging (Ï„=0.001)

## ğŸ“Š Anti-Catastrophic Forgetting Features

1. **Q-value Explosion Prevention**:
   - Reduced discount factor (0.95 vs 0.99)
   - Q-value clamping [-100, 100]
   - Gradient clipping (norm=1.0)
   - Proper network initialization

2. **Curriculum Learning**:
   - Smooth phase transitions with 4000-episode overlaps
   - Progressive difficulty: Random â†’ Heuristic â†’ Mixed
   - Reservoir sampling for experience diversity

3. **Strategic Learning Retention**:
   - Amplified intermediate rewards for visibility
   - Immediate tactical feedback
   - Anti-forgetting experience replay

## ğŸ”¬ Research Applications

This codebase is designed for studying:
- **Catastrophic Forgetting in Deep RL**: Comprehensive analysis and mitigation strategies
- **Curriculum Learning**: Progressive difficulty with smooth transitions
- **Strategic Game Learning**: Connect4-specific tactical decision making
- **Multi-agent Training Dynamics**: Agent behavior across different opponents

## ğŸ“ˆ Expected Results

With the implemented fixes, training demonstrates:
- âœ… Stable Q-values (range -20 to +20, no explosion)
- âœ… Smooth curriculum transitions (no sudden performance drops)
- âœ… Strategic learning retention (>50% win rate vs heuristic)
- âœ… Convergence without catastrophic oscillations

## Development Milestones

- âœ… **Milestone 1**: Core game engine + strategic agents
- âœ… **Milestone 2**: Tabular Q-learning implementation
- âœ… **Milestone 3**: Deep Q-Network with PyTorch
- âœ… **Milestone 4**: Double DQN with experience replay
- âœ… **Milestone 5**: Dueling DQN architecture
- âœ… **Milestone 6**: Anti-catastrophic forgetting system
- âœ… **Milestone 7**: Advanced training stability features

See `docs/` folder for detailed development documentation.

## ğŸ¤ Contributing

This project follows modular design principles:
- Extend existing components rather than creating new files
- Reference existing code patterns and architectures
- Maintain compatibility with current training pipelines
- Follow the guidelines in `CLAUDE.md`

## Dependencies

Core requirements:
- `torch`: PyTorch for neural networks and training
- `numpy`: Efficient array operations for game state
- `matplotlib`: Training visualization and plotting

See `requirements.txt` for complete dependency list.

## ğŸ† Project Achievements

- **Strategic AI**: Human-competitive Connect4 gameplay
- **Stable Training**: Solved Q-value explosion and catastrophic forgetting
- **Research Platform**: Ready for RL research and experimentation
- **Modular Architecture**: Extensible and maintainable codebase

---

*This implementation demonstrates advanced RL techniques applied to a classic game, serving as both a research platform and educational resource for understanding deep reinforcement learning challenges and solutions.*